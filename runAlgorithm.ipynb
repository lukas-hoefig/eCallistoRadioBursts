{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import copy \n",
    "import pickle \n",
    "from astropy.io import fits\n",
    "\n",
    "import download\n",
    "import stations\n",
    "import analysis\n",
    "import events\n",
    "import data\n",
    "import correlation\n",
    "import reference\n",
    "import const\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ALASKA-COHOE, ALASKA-COHOE, ALASKA-HAARP, ALASKA-HAARP, ALGERIA-CRAAG, ALMATY, AUSTRIA-Krumbach, AUSTRIA-OE3FLB, AUSTRIA-UNIGRAZ, Arecibo-Observatory, Arecibo-Observatory, Australia-ASSA, Australia-ASSA, Australia-LMRO, BIR, DENMARK, EGYPT-Alexandria, EGYPT-Alexandria, GLASGOW, HUMAIN, HURBANOVO, INDIA-GAURI, INDIA-Nashik, INDIA-OOTY, INDIA-OOTY, INDIA-UDAIPUR, INDONESIA, KASI, MEXART, MONGOLIA-UB, MRO, MRO, MRO, MRO, ROSWELL-NM, SOUTHAFRICA-SANSA, SPAIN-PERALEJOS, SPAIN-PERALEJOS, SWISS-HB9SCT, SWISS-IRSOL, SWISS-Landschlacht, SWISS-Landschlacht, SWISS-MUHEN, SWISS-MUHEN, TRIEST, URUGUAY]\n",
      "failed to open f:/programming/eCallistoRadioBursts/eCallistoData/2022/01/03/INDONESIA_20220103_233000_59.fit.gz\n",
      "failed to open f:/programming/eCallistoRadioBursts/eCallistoData/2022/01/02/INDONESIA_20220102_070000_59.fit.gz\n",
      "failed to open f:/programming/eCallistoRadioBursts/eCallistoData/2022/01/03/MEXART_20220103_180000_59.fit.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lukas\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\window\\rolling.py:321: UserWarning: Warning: converting a masked element to nan.\n",
      "  values = ensure_float64(values)\n",
      "f:\\programming\\eCallistoRadioBursts\\data.py:254: UserWarning: Warning: converting a masked element to nan.\n",
      "  arr = np.array(self.summedCurve)\n",
      "C:\\Users\\Lukas\\AppData\\Roaming\\Python\\Python38\\site-packages\\numpy\\lib\\nanfunctions.py:1212: UserWarning: Warning: converting a masked element to nan.\n",
      "  a = np.asanyarray(a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed to open f:/programming/eCallistoRadioBursts/eCallistoData/2022/01/03/INDONESIA_20220103_233000_59.fit.gz\n",
      "failed to open f:/programming/eCallistoRadioBursts/eCallistoData/2022/01/02/INDONESIA_20220102_070000_59.fit.gz\n",
      "failed to open f:/programming/eCallistoRadioBursts/eCallistoData/2022/01/03/MEXART_20220103_180000_59.fit.gz\n"
     ]
    }
   ],
   "source": [
    "_year = 2022\n",
    "_month = 1\n",
    "_day = 3\n",
    "_days = 2\n",
    "\n",
    "nobg = True\n",
    "bin_f = False\n",
    "bin_t = False\n",
    "flatten = True\n",
    "bin_t_w = 4\n",
    "flatten_w = 400\n",
    "r_w = 180\n",
    "\n",
    "limit = 0.6\n",
    "\n",
    "date_start = datetime(year=_year, month=_month, day=_day)\n",
    "time_step = timedelta(days=1)\n",
    "number_days = _days\n",
    "\n",
    "observatory = stations.getStations(date_start)\n",
    "print(observatory)\n",
    "\n",
    "for i in range(number_days):\n",
    "    date = date_start + time_step * i\n",
    "    year = date.year\n",
    "    month = date.month\n",
    "    day = date.day\n",
    "    download.downloadFullDay(date_start, station=observatory)\n",
    "    sets = []\n",
    "    for j in observatory:\n",
    "        sets.extend(data.listDataPointDay(date_start, station=j))\n",
    "    e_list = events.EventList([])\n",
    "    for set1 in range(len(sets)):\n",
    "        for set2 in range(set1 + 1, len(sets)):\n",
    "            data1_raw = copy.deepcopy(sets[set1])\n",
    "            data2_raw = copy.deepcopy(sets[set2])\n",
    "            data1, data2 = data.fitTimeFrameDataSample(data1_raw, data2_raw)\n",
    "            if data1 and data2:\n",
    "                corr = correlation.Correlation(data1, data2, day, _no_background=nobg, _bin_freq=bin_f,\n",
    "                                               _bin_time=bin_t, _flatten=flatten, _bin_time_width=bin_t_w,\n",
    "                                               _flatten_window=flatten_w, _r_window=r_w)\n",
    "                corr.calculatePeaks(_limit=limit)\n",
    "                e_list += corr.peaks\n",
    "            else:\n",
    "                pass\n",
    "    e_list.sort()\n",
    "    analysis.saveData(e_list, year, month, day)\n",
    "    #print(f\"\\n {date.year} {date.month} {date.day}\")\n",
    "    #print(\"mine\")\n",
    "    #print(e_list)\n",
    "    #print(\"reference SWPC\")\n",
    "    #print(reference.referenceSWPC(year, month, day))\n",
    "    #print(\"reference Monstein\")\n",
    "    #print(reference.referenceMonstein(year, month, day))\n",
    "    #print(\"reference Monstein with 2 or more stations\")\n",
    "    #print(reference.referenceMonstein2orMore(year, month, day))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.system('shutdown -s')\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file= const.pathDataDay(2022,1,1)+\"ALASKA-COHOE_20220101_181500_01.fit.gz\"\n",
    "stations.getStationFromFile(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_year = 2022\n",
    "_month = 1\n",
    "_day = 3 \n",
    "_days = 2\n",
    "\n",
    "nobg=True\n",
    "bin_f=False\n",
    "bin_t=False\n",
    "flatten=True\n",
    "bin_t_w=4\n",
    "flatten_w=400\n",
    "r_w=180\n",
    "spec_range = [45, 81]\n",
    "\n",
    "limit = 0.6\n",
    "\n",
    "date_start = datetime(year=_year, month=_month, day=_day)\n",
    "time_step = timedelta(days=1)\n",
    "number_days = _days\n",
    "\n",
    "observatory = [observatories.uni_graz, observatories.triest, observatories.swiss_landschlacht, observatories.oe3flb,\n",
    "               observatories.alaska_haarp, observatories.alaska_cohoe, observatories.roswell, observatories.bir,\n",
    "               observatories.indonesia, observatories.assa, observatories.swiss_muhen, observatories.swiss_hb9sct,\n",
    "               observatories.egypt_alexandria, observatories.arecibo, observatories.swiss_heiterswil, observatories.humain,\n",
    "               observatories.glasgow, observatories.greenland]\n",
    "\n",
    "events_day = []\n",
    "for i in range(number_days):\n",
    "    date = date_start + time_step * i\n",
    "    year = date.year\n",
    "    month = date.month\n",
    "    day = date.day\n",
    "    download.downloadFullDay(year, month, day, observatory)\n",
    "    stations = download.observatoriesAvailable(year, month, day)[1]\n",
    "    sets = []\n",
    "    for j in stations:\n",
    "        sets.extend(data.listDataPointDay(year, month, day, j, spec_range))\n",
    "    events = events.EventList([])\n",
    "    for set1 in range(len(sets)):\n",
    "        for set2 in range(set1 + 1, len(sets)):\n",
    "            data1_raw = copy.deepcopy(sets[set1])\n",
    "            data2_raw = copy.deepcopy(sets[set2])\n",
    "            data1, data2 = data.fitTimeFrameDataSample(data1_raw, data2_raw)\n",
    "            if data1 and data2:\n",
    "                corr = correlation.Correlation(data1, data2, day, _no_background=nobg, _bin_freq=bin_f,\n",
    "                                               _bin_time=bin_t, _flatten=flatten, _bin_time_width=bin_t_w,\n",
    "                                               _flatten_window=flatten_w, _r_window=r_w)\n",
    "                corr.calculatePeaks(_limit=limit)\n",
    "                events += corr.peaks\n",
    "            else:\n",
    "                pass\n",
    "    events.sort()\n",
    "    events_day.append(events)\n",
    "    #print(f\"\\n {date.year} {date.month} {date.day}\")\n",
    "    #print(\"mine\")\n",
    "    #print(events)\n",
    "    #print(\"reference SWPC\")\n",
    "    #print(reference.referenceSWPC(year, month, day))\n",
    "    #print(\"reference Monstein\")\n",
    "    #print(reference.referenceMonstein(year, month, day))\n",
    "    #print(\"reference Monstein with 2 or more stations\")\n",
    "    #print(reference.referenceMonstein2orMore(year, month, day))\n",
    "# return events_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 1\n",
    "for i in events_day:\n",
    "    print(r)\n",
    "    r += 1\n",
    "    for j in i:\n",
    "        print(j, j.stations)\n",
    "        # print(observatories.ObservatorySet(j.stations).getSet())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_day_checked = []\n",
    "for list_day in events_day:\n",
    "    e_list = analysis.EventList([])\n",
    "    for event in list_day:\n",
    "        obs = observatories.ObservatorySet(event.stations)\n",
    "        event_time = event.time_start\n",
    "        event_time_ahead = event_time - timedelta(minutes=15)\n",
    "        set_obs = obs.getSet()\n",
    "        for i in set_obs:\n",
    "            try:\n",
    "                dp11 = data.createFromTime(event_time.year, event_time.month, event_time.day, str(event.time_start), i[0], spec_range)\n",
    "                dp12 = data.createFromTime(event_time_ahead.year, event_time_ahead.month, event_time_ahead.day, str(event.time_start-timedelta(minutes=15)), i[0], spec_range)\n",
    "                dp21 = data.createFromTime(event_time.year, event_time.month, event_time.day, str(event.time_start), i[1], spec_range)\n",
    "                dp22 = data.createFromTime(event_time_ahead.year, event_time_ahead.month, event_time_ahead.day, str(event.time_start-timedelta(minutes=15)), i[1], spec_range)\n",
    "                dp1, dp2 = data.fitTimeFrameDataSample([sum([dp11,dp12])], [sum([dp21,dp22])])\n",
    "            except TypeError:\n",
    "                continue\n",
    "            except:                \n",
    "                dp1 = data.createFromTime(event_time.year, event_time.month, event_time.day, str(event.time_start), i[0], spec_range)     \n",
    "                dp2 = data.createFromTime(event_time.year, event_time.month, event_time.day, str(event.time_start), i[1], spec_range)       \n",
    "\n",
    "            cor = correlation.Correlation(dp1, dp2, day,  \n",
    "                                          _flatten=True, _bin_time=True, _bin_freq=True, _no_background=True,\n",
    "                                          _r_window=30)\n",
    "            cor.calculatePeaks()\n",
    "            e_list += cor.peaks\n",
    "    events_day_checked.append(e_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_day_checked = []\n",
    "for list_day in events_day:\n",
    "    e_list = analysis.EventList([])\n",
    "    for event in list_day:\n",
    "        obs = observatories.ObservatorySet(event.stations)\n",
    "        year = event.time_start.year\n",
    "        month = event.time_start.month\n",
    "        day = event.time_start.day\n",
    "        set_obs = obs.getSet()\n",
    "        for i in set_obs:\n",
    "            dp1 = data.createFromTime(year, month, day, str(event.time_start), i[0], spec_range)      # needs to search for the file before this one (if close to start fo file)  [ - timedelta(minutes=15)]\n",
    "            dp2 = data.createFromTime(year, month, day, str(event.time_start), i[1], spec_range)            # same here  , also data.fitTimeFrameDataSample(), because reasons\n",
    "            cor = correlation.Correlation(dp1, dp2, day,  \n",
    "                                          _flatten=True, _bin_time=True, _bin_freq=True, _no_background=True,\n",
    "                                          _r_window=30)\n",
    "            cor.calculatePeaks()\n",
    "            e_list += cor.peaks\n",
    "    events_day_checked.append(e_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_day_checked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _days = 28\n",
    "ref_swpc = [reference.referenceSWPC(year, month, i) for i in range(_day, _day + _days)]\n",
    "ref_monstein = [reference.referenceMonstein(year, month, i) for i in range(_day, _day + _days)]\n",
    "ref_monstein2p = [reference.referenceMonstein2orMore(year, month, i) for i in range(_day, _day + _days)]\n",
    "ref_swpc_strong = [analysis.EventList([j for j in i if j.burst_type == \"III/2\" or j.burst_type == \"III/3\"]) for i in ref_swpc]\n",
    "# events with burst_type.startswith(III/) or (II/)  ??\n",
    "\n",
    "events_num = sum(len(i) for i in events_day[:_days])\n",
    "\n",
    "events_swpc = sum(len(i)for i in ref_swpc[:_days])\n",
    "events_swpc_strong = sum(len(i)for i in ref_swpc_strong[:_days])\n",
    "\n",
    "events_monstein = sum(len(i)for i in ref_monstein[:_days])\n",
    "events_monstein2p = sum(len(i)for i in ref_monstein2p[:_days])\n",
    "\n",
    "false_positives_all = []\n",
    "missed_strong_all = []\n",
    "missed_swpc_all = []\n",
    "missed_monstein_all = []\n",
    "missed_monstein2p_all = []\n",
    "for i in range(_day, _day + _days):\n",
    "    false_positives = (events_day[i-1] - ref_swpc[i-1]) - ref_monstein[i-1]\n",
    "    false_positives_all.extend(false_positives)\n",
    "    print(f\"\\n{year} {month} {i}\\nfalse positives: \", false_positives)\n",
    "\n",
    "    missed_swpc = ref_swpc[i-1] - events_day[i-1]\n",
    "    print(f\"missed swpc:\", missed_swpc)\n",
    "    missed_swpc_all.extend(missed_swpc)\n",
    "\n",
    "    missed_monstein = ref_monstein[i-1] - events_day[i-1]\n",
    "    print(f\"missed monstein:\", missed_monstein)\n",
    "    missed_monstein_all.extend(missed_monstein)\n",
    "\n",
    "    missed_monstein2p = ref_monstein2p[i-1] - events_day[i-1]\n",
    "    print(f\"missed monstein measured at 2+ of my stations:\", missed_monstein2p)\n",
    "    missed_monstein2p_all.extend(missed_monstein2p)\n",
    "\n",
    "    missed_strong = ref_swpc_strong[i-1] - events_day[i-1]\n",
    "    print(f\"missed strong:\", missed_strong)\n",
    "    missed_strong_all.extend(missed_strong)\n",
    "\n",
    "print(f\"\\n------------------------------\\nEvents found: {events_num}\")\n",
    "print(f\"False positives: {len(false_positives_all)} ({len(false_positives_all)/events_num*100}%)\")\n",
    "print(f\"\\nFailed bursts SWPC: {len(missed_swpc_all)} ({len(missed_swpc_all)/events_swpc*100}%)\")\n",
    "print(f\"Failed bursts SWPC (strong): {len(missed_strong_all)} ({len(missed_strong_all)/events_swpc_strong*100}%)\")\n",
    "print(f\"Failed bursts Monstein: {len(missed_monstein_all)} ({len(missed_monstein_all)/events_monstein*100}%)\")\n",
    "print(f\"Failed bursts Monstein measured at >=2 stations: {len(missed_monstein2p_all)} ({len(missed_monstein2p_all)/events_monstein2p*100}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52279f95a71c719c29b46e6649e7c2848e29d27590516e424c9feb43371b1514"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
